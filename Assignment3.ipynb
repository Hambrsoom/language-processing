{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE*: Functions were built with the assumption that data files follow the tweet_id, text, q1_label, q2_label, q3_label, q4_label, q5_label, q6_label, q7_label format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare function to import data from TSV file\n",
    "import csv\n",
    "\n",
    "def importTSV(file_name):\n",
    "    tsv_file = open(file_name, encoding=\"utf8\")\n",
    "    read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "        \n",
    "    training_data = []\n",
    "    \n",
    "    for row in read_tsv:\n",
    "        training_data.append(row)\n",
    "        \n",
    "    # removing the first row because it contains \n",
    "    # ['tweet_id', 'text', 'q1_label', 'q2_label', 'q3_label', 'q4_label', 'q5_label', 'q6_label', 'q7_label']\n",
    "    if \"training\" in file_name:\n",
    "        training_data.pop(0)\n",
    "        \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare function to convert data to lowercase\n",
    "\n",
    "def convertToLowerCase(training_data):\n",
    "    for row in training_data:\n",
    "        row[1] = row[1].lower()\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes count 247\n",
      "no count 152\n",
      "size of original V 4304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '!!',\n",
       " '\"chinese',\n",
       " '\"i',\n",
       " '\"it\\'s',\n",
       " '\"leadership\"',\n",
       " '\"others\"',\n",
       " '\"scare\"',\n",
       " '\"the',\n",
       " '\"why',\n",
       " '\"wuhan',\n",
       " '#',\n",
       " '#aids',\n",
       " '#b52questions',\n",
       " '#benice',\n",
       " '#bluewave2020',\n",
       " '#btsarmy',\n",
       " '#btslovemyself',\n",
       " '#ccpvirus',\n",
       " '#cdctravelnotices:',\n",
       " '#chanyeol,',\n",
       " '#china',\n",
       " '#china.',\n",
       " '#chinesevirus',\n",
       " '#christians',\n",
       " '#commonsense',\n",
       " '#corona',\n",
       " '#corona.',\n",
       " '#coronaalert',\n",
       " '#coronaoutbreak',\n",
       " '#coronaoutbreak.',\n",
       " '#coronaoutbreak?',\n",
       " '#coronavirus',\n",
       " '#coronavirus,',\n",
       " '#coronavirus.',\n",
       " '#coronavirus.he',\n",
       " '#coronavirus:',\n",
       " '#coronavirus?',\n",
       " '#coronavirus??',\n",
       " '#coronaviruschallenge',\n",
       " '#coronaviruschina',\n",
       " '#coronavirusindia',\n",
       " '#coronavirusinindia',\n",
       " '#coronavirusoutbreak',\n",
       " '#coronavirusoutbreak,',\n",
       " '#coronavirusoutbreak.',\n",
       " '#coronavirusoutbreak?',\n",
       " '#coronaviruspakistan.',\n",
       " '#coronaviruspandemic',\n",
       " '#coronavirusspread',\n",
       " '#coronavirusupdate',\n",
       " '#coronavirusupdates',\n",
       " '#coronavirususa',\n",
       " '#coronawarriors',\n",
       " '#coronvirus',\n",
       " '#covid19',\n",
       " '#covid19!',\n",
       " '#covid19,',\n",
       " '#covid19-facepalm',\n",
       " '#covid19-induced',\n",
       " '#covid19.',\n",
       " '#covid19.https://t.co/kkprjkwait',\n",
       " '#covid19india',\n",
       " '#covid19nigeria',\n",
       " '#covid19on',\n",
       " '#covid19out',\n",
       " '#covid2019',\n",
       " '#covid_19',\n",
       " '#covid_19.',\n",
       " '#covidance',\n",
       " '#covidã\\x83¼19',\n",
       " '#covidー19',\n",
       " '#dial100',\n",
       " '#droolingbloomberg',\n",
       " '#endviolence',\n",
       " '#exo',\n",
       " '#factsfirst',\n",
       " '#factsmatter',\n",
       " '#factsnotfear',\n",
       " '#fake',\n",
       " '#fakenews',\n",
       " '#fakenews.',\n",
       " '#fakenews?',\n",
       " '#fakenewsmedia',\n",
       " '#fakescience',\n",
       " '#false',\n",
       " '#fearporn,',\n",
       " '#fisa',\n",
       " '#flattenthecurve',\n",
       " '#forcedisolation',\n",
       " '#foxclassaction',\n",
       " '#germaphobetrump',\n",
       " '#getmeppe',\n",
       " '#hantavirus',\n",
       " '#hongkong',\n",
       " '#hongkongers',\n",
       " '#humanrights',\n",
       " '#hypocrite',\n",
       " '#india-',\n",
       " '#indiafightscorona',\n",
       " '#jimbakker',\n",
       " '#justicefordrcarlos',\n",
       " '#justsowrongjoe',\n",
       " '#kag',\n",
       " '#kag2020',\n",
       " '#kai',\n",
       " '#kckpd',\n",
       " '#korona',\n",
       " '#koronawirus',\n",
       " '#landslide2020',\n",
       " '#lay,',\n",
       " '#lisalandau,',\n",
       " '#maga',\n",
       " '#makkah',\n",
       " '#marr',\n",
       " '#michaelbloomberg.',\n",
       " '#militarytrain',\n",
       " '#minimikebloomberg',\n",
       " '#mog☘️',\n",
       " '#n95masks',\n",
       " '#namaste',\n",
       " '#nationalguard',\n",
       " '#nofakenews',\n",
       " '#norumors',\n",
       " '#notahoax',\n",
       " '#notokayjoe',\n",
       " '#notoviptesting',\n",
       " '#numbskulls',\n",
       " '#nurses',\n",
       " '#onevoice1',\n",
       " '#pakistan',\n",
       " '#patriots,',\n",
       " '#pharmafauci',\n",
       " '#pibfactcheck:',\n",
       " '#qanon',\n",
       " '#qatar',\n",
       " '#racist',\n",
       " '#ridge',\n",
       " '#rwanda',\n",
       " '#safetysaves',\n",
       " '#sars',\n",
       " '#shalomð\\x9f\\x87®ð\\x9f\\x87±#namasteð\\x9f\\x87®ð\\x9f\\x87³',\n",
       " '#shareinformation',\n",
       " '#shareknowledge',\n",
       " '#sickpayforall',\n",
       " '#singapore.',\n",
       " '#standardofcare',\n",
       " '#stayathome',\n",
       " '#stayathomechallenge',\n",
       " '#stayconnected',\n",
       " '#stayhomestaysafe',\n",
       " '#staystrong_army',\n",
       " '#suho',\n",
       " '#taiwan’s',\n",
       " '#takingoncorona',\n",
       " '#thursdaymotivation',\n",
       " '#thursdaythoughts',\n",
       " '#trump2020',\n",
       " '#trumpliespeopledie',\n",
       " '#trumppandemic',\n",
       " '#trumptownhall',\n",
       " '#trumpvirus',\n",
       " '#uae',\n",
       " '#video:',\n",
       " '#votebluetosaveamerica',\n",
       " '#walangpasok',\n",
       " '#watch',\n",
       " '#whatsapp',\n",
       " '#wuhan',\n",
       " '#wuhan.',\n",
       " '#wuhancoronavirus,',\n",
       " '#wwg1wga',\n",
       " '#xijinping',\n",
       " '#zhangyixing',\n",
       " '#ì\\x88\\x98í\\x98¸',\n",
       " '#ì\\x97\\x91ì\\x86\\x8c',\n",
       " '#ì°¬ì\\x97´',\n",
       " '#우리의응원을이어주세요',\n",
       " '#힘내세요_아미',\n",
       " '$00.38',\n",
       " '$1,000,000',\n",
       " '$1.4m',\n",
       " '$57',\n",
       " '&',\n",
       " '&amp;',\n",
       " '&gt;&gt;33',\n",
       " '&lt;3',\n",
       " \"'16\",\n",
       " \"'beat\",\n",
       " \"'benign\",\n",
       " \"'fake\",\n",
       " \"'look,\",\n",
       " \"'not\",\n",
       " \"'russia\",\n",
       " \"'that's\",\n",
       " \"'the\",\n",
       " \"'we\",\n",
       " \"'why\",\n",
       " \"('scarring')\",\n",
       " '(allegedly),',\n",
       " '(and',\n",
       " '(borrowed)',\n",
       " \"(cont'd)\",\n",
       " '(coronavirus',\n",
       " '(including',\n",
       " '(people)',\n",
       " '(plaquenil)',\n",
       " '(severe',\n",
       " '(stories',\n",
       " '(unintentional)',\n",
       " '(wait',\n",
       " '(which',\n",
       " '(with',\n",
       " '*5,000*',\n",
       " '*hoarding',\n",
       " '*knowing*',\n",
       " '*that*',\n",
       " '*will*',\n",
       " '+',\n",
       " '+91',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '-80%',\n",
       " '-95,270',\n",
       " '-@drtedros',\n",
       " '-@who',\n",
       " '-countries',\n",
       " '-kobe',\n",
       " '-schools',\n",
       " '-tom',\n",
       " '-we’d',\n",
       " '-“it',\n",
       " '.',\n",
       " '...',\n",
       " '....',\n",
       " '.005%',\n",
       " '.@nationalnurses',\n",
       " '.@realdonaldtrump',\n",
       " '.@realdonaldtrump,',\n",
       " '.@senschumer',\n",
       " '/',\n",
       " '0.1%',\n",
       " '0”',\n",
       " '1',\n",
       " '1%',\n",
       " \"1%,'\",\n",
       " '1%.\"',\n",
       " '1,000',\n",
       " '1-2',\n",
       " '1.',\n",
       " '1/',\n",
       " '1/n',\n",
       " '10',\n",
       " '10,000',\n",
       " '10.',\n",
       " '100%',\n",
       " '100,000',\n",
       " '10k',\n",
       " '10mins',\n",
       " '11',\n",
       " '11259',\n",
       " '11:00',\n",
       " '12,000',\n",
       " '124',\n",
       " '13',\n",
       " '131',\n",
       " '14',\n",
       " '14th',\n",
       " '15',\n",
       " '16',\n",
       " '160k',\n",
       " '17',\n",
       " '17.',\n",
       " '170',\n",
       " '18',\n",
       " '1k',\n",
       " '1st',\n",
       " '2',\n",
       " '2%',\n",
       " '2)',\n",
       " '2.',\n",
       " '2/26',\n",
       " '2/28',\n",
       " '2/4/20.',\n",
       " '20',\n",
       " '20.',\n",
       " '2014',\n",
       " '2016',\n",
       " '2019',\n",
       " '2019)',\n",
       " '2019...',\n",
       " '2020',\n",
       " '2020,',\n",
       " '2020:',\n",
       " '2022',\n",
       " '21',\n",
       " '215',\n",
       " '22',\n",
       " '225',\n",
       " '23',\n",
       " '230k',\n",
       " '233',\n",
       " '24',\n",
       " '24-feb',\n",
       " '25',\n",
       " '25..',\n",
       " '25th',\n",
       " '26.',\n",
       " '28),',\n",
       " '29',\n",
       " '29.',\n",
       " '2pm.',\n",
       " '3',\n",
       " \"3%'\",\n",
       " '3,281',\n",
       " '3,308',\n",
       " '3,398',\n",
       " '3.',\n",
       " '3.4%',\n",
       " '3.4%:',\n",
       " '3/12',\n",
       " '3/16:',\n",
       " '3/22:',\n",
       " '30',\n",
       " '300.000',\n",
       " '31',\n",
       " '31,',\n",
       " '310,000.',\n",
       " '34x',\n",
       " '35k',\n",
       " '36.',\n",
       " '37',\n",
       " '38',\n",
       " '38-year-old',\n",
       " '38-yr-old',\n",
       " '3800',\n",
       " '3g',\n",
       " '3pm',\n",
       " '3rd',\n",
       " '4',\n",
       " '4.',\n",
       " '40',\n",
       " '400',\n",
       " '4747',\n",
       " '48hrs',\n",
       " '5',\n",
       " '5,',\n",
       " '5.',\n",
       " '50',\n",
       " '50%',\n",
       " '505,500,000',\n",
       " '509',\n",
       " '53,981',\n",
       " '55,672',\n",
       " '56',\n",
       " '58',\n",
       " '5:',\n",
       " '6',\n",
       " '6.',\n",
       " '60',\n",
       " '60s.',\n",
       " '65+',\n",
       " '6th',\n",
       " '7',\n",
       " '7.',\n",
       " '70%',\n",
       " '7pm',\n",
       " '8.',\n",
       " '80%',\n",
       " '87997',\n",
       " '88%',\n",
       " '8:00',\n",
       " '9',\n",
       " '9.',\n",
       " '909',\n",
       " '96,950',\n",
       " '99%',\n",
       " ':',\n",
       " ':-/',\n",
       " ';',\n",
       " '<link>',\n",
       " '?',\n",
       " '@',\n",
       " \"@aoc's\",\n",
       " '@axios',\n",
       " '@bbcworld',\n",
       " '@bts_twt',\n",
       " '@cdcgov',\n",
       " '@cnbc',\n",
       " \"@cnn's\",\n",
       " '@cpac',\n",
       " '@cpmumbaipolice',\n",
       " '@drtedros',\n",
       " '@eafifadirect',\n",
       " '@easportsfifa',\n",
       " '@fema',\n",
       " '@fifacom',\n",
       " '@fifacom_fr',\n",
       " '@fifamedia',\n",
       " '@fifaworldcup',\n",
       " '@fifaworldcup.',\n",
       " '@fifaworldcup_fr',\n",
       " '@gavinnewsom',\n",
       " '@gop',\n",
       " '@gopleader',\n",
       " '@govpritzker',\n",
       " '@hbo',\n",
       " '@hhsgov',\n",
       " '@houseforeign',\n",
       " '@iam_brookesmith',\n",
       " '@jaketapper',\n",
       " '@jhusystems',\n",
       " '@juditvarga_eu',\n",
       " '@lapublichealth',\n",
       " '@layzhang',\n",
       " '@mbuhari’s',\n",
       " '@mohfw_india',\n",
       " '@msnbc\\u2069',\n",
       " '@narendramodi',\n",
       " '@nationalnurses',\n",
       " '@netanyahu',\n",
       " '@nygovcuomo!',\n",
       " '@nytimes',\n",
       " '@nytimes,',\n",
       " '@pmoindia',\n",
       " '@potus',\n",
       " '@priyankagandhi',\n",
       " '@realdonaldtrump',\n",
       " '@realdonaldtrump.',\n",
       " '@secpompeo',\n",
       " '@senatedems',\n",
       " '@senatemajldr,',\n",
       " '@speakerpelosi',\n",
       " '@statedept',\n",
       " '@statedept:',\n",
       " '@surgeon_general',\n",
       " '@thespinofftv',\n",
       " '@thespybrief',\n",
       " '@tomfriedman',\n",
       " '@vanurse1212',\n",
       " '@vicenews.',\n",
       " '@vinodkapri',\n",
       " '@vp',\n",
       " '@waltshaub',\n",
       " '@washingtonpost',\n",
       " '@weareoneexo',\n",
       " '@whitehouse',\n",
       " '@wikipedia',\n",
       " '@worksafeca',\n",
       " '@xtotl',\n",
       " '[bye',\n",
       " '[tweeting',\n",
       " 'a',\n",
       " 'a...tennis',\n",
       " 'abandoned',\n",
       " 'abba',\n",
       " 'abbott',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abortions??',\n",
       " 'about',\n",
       " 'about.',\n",
       " 'abrahamic',\n",
       " 'abroad.',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abt',\n",
       " 'abundance',\n",
       " 'academics,',\n",
       " 'accelerating',\n",
       " 'accept',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'access',\n",
       " 'accident,',\n",
       " 'accompanying',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accounts,',\n",
       " 'accurate',\n",
       " 'accurate.',\n",
       " 'acquired',\n",
       " 'across',\n",
       " 'act',\n",
       " 'action',\n",
       " 'action.',\n",
       " 'actively',\n",
       " 'activities',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'actually,',\n",
       " 'acute',\n",
       " 'adams:',\n",
       " 'added',\n",
       " 'addressing',\n",
       " 'adequate',\n",
       " 'adham.',\n",
       " 'adm.',\n",
       " 'admin',\n",
       " 'administration',\n",
       " 'administration,',\n",
       " 'administrationâ\\x80\\x99s',\n",
       " 'admits',\n",
       " 'admits:',\n",
       " 'admitted',\n",
       " 'ado,',\n",
       " 'adopt',\n",
       " 'adult',\n",
       " 'advance.',\n",
       " 'advice',\n",
       " 'advice⚠️',\n",
       " 'advised',\n",
       " 'advisory',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'afk.',\n",
       " 'afraid',\n",
       " 'africa,',\n",
       " 'after',\n",
       " 'ag',\n",
       " 'again',\n",
       " 'again.',\n",
       " 'again.<link>',\n",
       " 'again:',\n",
       " 'against',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'ages,”',\n",
       " 'aggresively',\n",
       " 'aggressive',\n",
       " 'aggressive,',\n",
       " 'ago',\n",
       " 'ago!!!',\n",
       " 'ago,',\n",
       " 'ago.',\n",
       " 'ago:',\n",
       " 'agree',\n",
       " 'ag’s',\n",
       " 'ahead',\n",
       " 'aiims',\n",
       " 'aiims,',\n",
       " \"ain't\",\n",
       " 'ain’t',\n",
       " 'air',\n",
       " 'air,',\n",
       " 'airline',\n",
       " 'airlines.',\n",
       " 'airports.',\n",
       " 'alarm]',\n",
       " 'alberta',\n",
       " 'albums',\n",
       " 'albums.',\n",
       " 'alert',\n",
       " 'alert:',\n",
       " 'alert‼️‼️‼️',\n",
       " 'ali',\n",
       " 'aliens,',\n",
       " 'all',\n",
       " 'all,',\n",
       " 'all-government',\n",
       " 'allah,',\n",
       " 'allegedly',\n",
       " 'alleging',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'already',\n",
       " 'also',\n",
       " 'also,',\n",
       " 'altnews',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amazing!',\n",
       " 'america',\n",
       " 'america.',\n",
       " 'america:',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'americans:',\n",
       " 'amnesty',\n",
       " 'among',\n",
       " 'amount',\n",
       " 'amvca',\n",
       " 'an',\n",
       " 'analysis:',\n",
       " 'and',\n",
       " 'angelenos',\n",
       " 'angeles,',\n",
       " 'angry',\n",
       " 'animal',\n",
       " \"animal's\",\n",
       " 'announce,',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announcement:',\n",
       " 'announces',\n",
       " 'announcing',\n",
       " 'anong',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'anti-inflammatory',\n",
       " 'anti-malaria',\n",
       " 'anti-viral',\n",
       " 'antiblack',\n",
       " 'antiviral',\n",
       " 'anxiety',\n",
       " 'anxiety.',\n",
       " 'anxiety?',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyone',\n",
       " 'anything.',\n",
       " 'anyways.',\n",
       " 'anywhere',\n",
       " 'apartments',\n",
       " 'api',\n",
       " 'apocalypse',\n",
       " 'apologize',\n",
       " 'apologized',\n",
       " 'apparent',\n",
       " 'apparently.',\n",
       " 'appeals',\n",
       " 'applaud',\n",
       " 'applies',\n",
       " 'approach',\n",
       " 'approval',\n",
       " 'approx.',\n",
       " 'april',\n",
       " 'arabia,',\n",
       " 'araw',\n",
       " 'are',\n",
       " 'are...really',\n",
       " 'are/were',\n",
       " 'are?',\n",
       " 'area',\n",
       " 'area,',\n",
       " 'area.',\n",
       " 'areas',\n",
       " 'areas.',\n",
       " \"aren't\",\n",
       " 'aren’t',\n",
       " 'argument',\n",
       " 'around',\n",
       " 'arranging',\n",
       " 'arrest',\n",
       " 'arrest?',\n",
       " 'arrhythmias',\n",
       " 'arrivals.â\\x80\\x9d',\n",
       " 'arriving',\n",
       " 'article',\n",
       " 'articles,',\n",
       " 'artificially',\n",
       " 'as',\n",
       " 'asap',\n",
       " 'asian',\n",
       " 'asians',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'ass',\n",
       " 'assam',\n",
       " 'assistant',\n",
       " 'assisting',\n",
       " 'associated',\n",
       " 'association',\n",
       " 'assure',\n",
       " 'astoundingly',\n",
       " 'asymptomatic',\n",
       " 'at',\n",
       " 'atlanta',\n",
       " 'attached',\n",
       " 'attacked',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attended',\n",
       " 'attendee',\n",
       " 'attendees',\n",
       " 'attention',\n",
       " 'attribution',\n",
       " 'authentic',\n",
       " 'authorities',\n",
       " 'autopsies',\n",
       " 'available',\n",
       " 'available.',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'awakening.',\n",
       " 'aware',\n",
       " 'aware.',\n",
       " 'away',\n",
       " 'away.',\n",
       " 'away”',\n",
       " 'awhile.',\n",
       " 'azithromycin',\n",
       " 'back',\n",
       " 'back.',\n",
       " 'background',\n",
       " 'bad',\n",
       " 'bad!',\n",
       " 'bad,',\n",
       " 'bag',\n",
       " 'bags',\n",
       " 'bail',\n",
       " 'bailout',\n",
       " 'bakker',\n",
       " 'balconies',\n",
       " 'ban',\n",
       " 'bankruptcy',\n",
       " 'banned',\n",
       " 'bans',\n",
       " 'bare',\n",
       " 'barista',\n",
       " 'barr',\n",
       " 'barrier',\n",
       " 'bars',\n",
       " 'based',\n",
       " 'batman',\n",
       " 'batteries',\n",
       " 'battle',\n",
       " 'battling',\n",
       " 'bay',\n",
       " 'bcos',\n",
       " 'be',\n",
       " 'be.”',\n",
       " 'beaches',\n",
       " 'beat',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becoming',\n",
       " 'bed',\n",
       " 'beds',\n",
       " 'been',\n",
       " 'beer',\n",
       " 'before',\n",
       " 'began,',\n",
       " 'begging',\n",
       " 'beginning',\n",
       " 'behind',\n",
       " 'beijingâ\\x80\\x99s',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'believing',\n",
       " 'belong',\n",
       " 'belonging',\n",
       " 'below',\n",
       " 'below.',\n",
       " 'bending',\n",
       " 'benefit.',\n",
       " 'benefits',\n",
       " 'benjamin',\n",
       " 'berlin.',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beware',\n",
       " 'bias.',\n",
       " 'big',\n",
       " 'big,',\n",
       " 'bigger.✨',\n",
       " 'biggest',\n",
       " 'bigot.',\n",
       " 'bigoted',\n",
       " 'bigotry',\n",
       " 'bihar',\n",
       " 'bihar.',\n",
       " 'bihar:',\n",
       " 'bihar:one',\n",
       " 'bihun',\n",
       " 'bill',\n",
       " 'bill,',\n",
       " 'bill.',\n",
       " 'billion',\n",
       " 'binay',\n",
       " 'biology',\n",
       " 'bipartisan',\n",
       " 'birds.',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bjp.',\n",
       " 'black',\n",
       " 'blame',\n",
       " 'blamed',\n",
       " 'blaming🇺🇸4',\n",
       " 'blanket',\n",
       " 'blew',\n",
       " 'blind',\n",
       " 'blood',\n",
       " 'body',\n",
       " 'bogus',\n",
       " 'bombarding',\n",
       " 'bonespurs',\n",
       " 'book',\n",
       " 'bookmark/come',\n",
       " 'boomer',\n",
       " 'border',\n",
       " 'borders',\n",
       " 'boris',\n",
       " 'boss',\n",
       " 'botched',\n",
       " 'both',\n",
       " 'bout',\n",
       " 'box',\n",
       " 'boxes',\n",
       " 'brady',\n",
       " 'brain.',\n",
       " 'brat.',\n",
       " 'break.',\n",
       " 'breaking',\n",
       " 'breaking:',\n",
       " 'breakthrough:',\n",
       " 'breath',\n",
       " 'breath.',\n",
       " 'breath?',\n",
       " 'breathing',\n",
       " 'brett',\n",
       " 'bridge',\n",
       " 'brief',\n",
       " 'briefing',\n",
       " 'briefings',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'britain”',\n",
       " 'bro',\n",
       " 'broadcasts',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'bronchitis.',\n",
       " 'brought',\n",
       " 'brunch.',\n",
       " 'bryant',\n",
       " 'bts’',\n",
       " 'buccaneer',\n",
       " 'buick',\n",
       " 'building',\n",
       " 'buildings,',\n",
       " 'built',\n",
       " 'bullshit',\n",
       " 'buloh',\n",
       " 'bunch',\n",
       " 'bunuh.',\n",
       " 'bureau,',\n",
       " 'burger',\n",
       " 'bus',\n",
       " 'buses',\n",
       " 'business',\n",
       " 'businesses',\n",
       " 'bussing',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'butt',\n",
       " 'buying',\n",
       " 'buying.',\n",
       " 'buys',\n",
       " 'by',\n",
       " 'ca.',\n",
       " 'cali',\n",
       " 'california',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'callousness',\n",
       " 'came',\n",
       " 'cameras',\n",
       " 'campaign',\n",
       " 'camps”',\n",
       " 'can',\n",
       " 'can!',\n",
       " \"can't\",\n",
       " 'can,',\n",
       " 'cancel',\n",
       " 'canceled',\n",
       " 'cancelled!',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'canâ\\x80\\x99t',\n",
       " 'can’t',\n",
       " 'capes,',\n",
       " 'cardinal,',\n",
       " 'cardiology',\n",
       " 'cardi’s',\n",
       " 'care',\n",
       " 'care!',\n",
       " 'care.',\n",
       " 'career',\n",
       " 'careful.',\n",
       " 'cares',\n",
       " 'caring',\n",
       " 'carlos',\n",
       " 'carried',\n",
       " 'carriers,',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'case,',\n",
       " 'case.',\n",
       " 'cases',\n",
       " 'cases.',\n",
       " 'cashed',\n",
       " 'catastrophic',\n",
       " 'catch',\n",
       " 'catching',\n",
       " 'cats',\n",
       " 'caught',\n",
       " 'causality',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'causes',\n",
       " 'causing',\n",
       " 'caution,',\n",
       " 'cc-by-sa',\n",
       " 'cdc',\n",
       " 'cdc,',\n",
       " 'cease',\n",
       " 'celebrate',\n",
       " 'celebrities',\n",
       " 'celia',\n",
       " 'center',\n",
       " 'center,',\n",
       " 'centre',\n",
       " 'centre.',\n",
       " 'chairman',\n",
       " 'challenge',\n",
       " 'challenge!',\n",
       " 'challenge.',\n",
       " 'challenges',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'charge',\n",
       " 'chart',\n",
       " 'chat',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'check.',\n",
       " 'check?',\n",
       " 'chicago',\n",
       " 'chief',\n",
       " 'chief:',\n",
       " 'child',\n",
       " 'child...',\n",
       " 'chill',\n",
       " 'china',\n",
       " 'china,',\n",
       " 'china.',\n",
       " 'china:',\n",
       " 'china?',\n",
       " 'chinese',\n",
       " 'chloroquine',\n",
       " 'choices',\n",
       " 'choiceâ\\x80\\x9d',\n",
       " 'choke',\n",
       " 'chornically-ill,',\n",
       " 'chronic',\n",
       " 'circling',\n",
       " 'circulated',\n",
       " 'circulating',\n",
       " 'cite',\n",
       " 'cities',\n",
       " 'cities.',\n",
       " 'citizen.',\n",
       " 'citizen.do',\n",
       " 'citizens',\n",
       " 'citizens,',\n",
       " 'citizenship',\n",
       " 'city',\n",
       " 'city.',\n",
       " 'civil',\n",
       " 'claim',\n",
       " 'claimed',\n",
       " 'claims',\n",
       " 'clap',\n",
       " 'clarify?',\n",
       " 'clarity:',\n",
       " 'class',\n",
       " 'class.',\n",
       " 'classes/work',\n",
       " 'classes/work?',\n",
       " 'classified',\n",
       " 'clean',\n",
       " 'cleaning',\n",
       " 'cleans',\n",
       " 'clear',\n",
       " 'clear,',\n",
       " 'clear:',\n",
       " 'clearly',\n",
       " 'click',\n",
       " 'climate',\n",
       " 'clinic',\n",
       " 'clinical',\n",
       " 'clip',\n",
       " 'clock',\n",
       " 'close',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare function to build ORIGINAL VOCABULARY\n",
    "\n",
    "def buildOriginalVocabulary(training_data):\n",
    "    vocab = set()\n",
    "    \n",
    "    for row in training_data:\n",
    "        for word in row[1].split():\n",
    "            vocab.add(word)\n",
    "\n",
    "    vocab = list(vocab)\n",
    "    \n",
    "    # Why are we sorting here?\n",
    "    vocab.sort()\n",
    "    \n",
    "    return vocab\n",
    "            \n",
    "training_data = importTSV(\"covid_training.tsv\")\n",
    "\n",
    "countYes = 0\n",
    "countNo = 0\n",
    "\n",
    "for row in training_data:\n",
    "    if(row[2] == 'yes'):\n",
    "        countYes += 1\n",
    "    else:\n",
    "        countNo += 1\n",
    "\n",
    "training_data = convertToLowerCase(training_data)\n",
    "buildOriginalVocabulary(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare function to build FILTERED VOCABULARY (only words appearing twice)\n",
    "\n",
    "def buildFilteredVocabulary(training_data):\n",
    "    vocab = {}\n",
    "    for row in training_data:\n",
    "        for word in row[1].split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1\n",
    "            else:\n",
    "                vocab[word] = vocab[word] + 1\n",
    "    \n",
    "    for word in list(vocab):\n",
    "        if vocab[word] == 1:\n",
    "            vocab.pop(word, None) \n",
    "    \n",
    "    vocab = list(vocab)\n",
    "    # why are we sorting here ?\n",
    "    vocab.sort()\n",
    "    \n",
    "    return vocab\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "training_data = importTSV(\"covid_training.tsv\")\n",
    "training_data = convertToLowerCase(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of original V 4304\n"
     ]
    }
   ],
   "source": [
    "# get original vocab\n",
    "og_vocab = buildOriginalVocabulary(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of filtered V 1193\n"
     ]
    }
   ],
   "source": [
    "# get filtered vocab\n",
    "filtered_vocab = buildFilteredVocabulary(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainNaiveBayesAlgorithm(labels, vocab, training_data, smoothing_value):\n",
    "    \n",
    "    cond_probs = {}\n",
    "        \n",
    "    for label in labels: #class in classes, ie yes/no\n",
    "        \n",
    "        # init cond_probs\n",
    "        cond_probs[label] = {}\n",
    "        for word in vocab:\n",
    "            cond_probs[label][word] = smoothing_value\n",
    "            \n",
    "        total_word_count_for_label = 0\n",
    "        \n",
    "        # get word count for each word in this label\n",
    "        for data in training_data: # get prob of word given label\n",
    "            \n",
    "            if data[2] == label: # check data that match label\n",
    "\n",
    "                for word in data[1].split(): # iterate through words of document\n",
    "                    if word in vocab: # only count words in vocabulary\n",
    "                        total_word_count_for_label = total_word_count_for_label + 1        \n",
    "                        cond_probs[label][word] = cond_probs[label][word] + 1\n",
    "        \n",
    "        # divide specific word count by total word count for this label\n",
    "        for word in cond_probs[label]:\n",
    "            cond_probs[label][word] = cond_probs[label][word]/(total_word_count_for_label + (smoothing_value*len(vocab)))        \n",
    "\n",
    "    probs = {}\n",
    "            \n",
    "    for label in labels: # get prob of label\n",
    "        count = 0\n",
    "        for data in training_data:\n",
    "            if data[2] == label:\n",
    "                count = count +1\n",
    "                \n",
    "        probs[label] =  count/len(training_data)\n",
    "    \n",
    "    return cond_probs, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestNaiveBayesAlgorithm(labels, probs, cond_probs, vocab, data_text):\n",
    "    scores = {}\n",
    "    for label in labels:\n",
    "        scores[label] = math.log10(probs[label])\n",
    "        for word in data_text.split():\n",
    "            # words found in test vocab, but not in training vocab, then discard\n",
    "            if word in vocab:\n",
    "                scores[label] = scores[label] + math.log10(cond_probs[label][word])\n",
    "                \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_probs, probs = TrainNaiveBayesAlgorithm(['yes', 'no'], og_vocab, training_data, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes count 33\n",
      "no count 22\n"
     ]
    }
   ],
   "source": [
    "testing_data = importTSV(\"covid_test_public.tsv\")\n",
    "\n",
    "countYes = 0\n",
    "countNo = 0\n",
    "\n",
    "for row in testing_data:\n",
    "    if(row[2] == 'yes'):\n",
    "        countYes += 1\n",
    "    else:\n",
    "        countNo += 1\n",
    "        \n",
    "print(\"yes count\", countYes)\n",
    "print(\"no count\", countNo)\n",
    "\n",
    "testing_data = convertToLowerCase(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the original vocabulary results in a performance of 61.82%, whereas the filtered vocabulary results in a performance of 65.45%. Therefore, we notice that the original vocabulary overfits the training data, and the filtered vocabulary is more generalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with training data and test on testing_data, and output trace to file\n",
    "\n",
    "def NaiveBayesAlgorithmTestPerformanceWithTraceOutputToFile(training_data, testing_data, smoothing_value, which_vocab):\n",
    "    \n",
    "    labels = ['yes', 'no']\n",
    "    \n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "    \n",
    "    file_name = \"\"\n",
    "    \n",
    "    vocab = []\n",
    "    \n",
    "    if which_vocab == 'original':\n",
    "        vocab = buildOriginalVocabulary(training_data)\n",
    "        file_name = \"trace_NB-BOW-OV.txt\"\n",
    "    elif which_vocab == 'filtered':\n",
    "        vocab = buildFilteredVocabulary(training_data)\n",
    "        file_name = \"trace_NB-BOW-FV.txt\"\n",
    "    else:\n",
    "        assert(False)\n",
    "    \n",
    "    cond_probs, probs = TrainNaiveBayesAlgorithm(labels, vocab, training_data, smoothing_value)\n",
    "\n",
    "    f = open(file_name, 'w')\n",
    "    \n",
    "    for data in testing_data:\n",
    "        scores = TestNaiveBayesAlgorithm(labels, probs, cond_probs, vocab, data[1])\n",
    "        \n",
    "        yes_no_label = 'no'\n",
    "        correct_status = 'wrong'\n",
    "        \n",
    "        isYes = False\n",
    "        if scores['yes']>scores['no']:\n",
    "            yes_no_label = 'yes'\n",
    "            isYes = True\n",
    "            \n",
    "        if isYes and data[2] == 'yes':\n",
    "            correct_count = correct_count + 1\n",
    "            correct_status = 'correct'\n",
    "        elif not isYes and data[2] == 'no':\n",
    "            correct_count = correct_count + 1\n",
    "            correct_status = 'correct'\n",
    "        else:\n",
    "            incorrect_count = incorrect_count + 1\n",
    "            \n",
    "        f.write(str(data[0]) + \"  \" + yes_no_label + \"  \" + str(scores[yes_no_label]) + \"  \" + data[2] + \"  \" + correct_status + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of filtered V 1193\n",
      "size of original V 4304\n"
     ]
    }
   ],
   "source": [
    "NaiveBayesAlgorithmTestPerformanceWithTraceOutputToFile(training_data, testing_data, 0.01, 'filtered')\n",
    "NaiveBayesAlgorithmTestPerformanceWithTraceOutputToFile(training_data, testing_data, 0.01, 'original')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Evaluation Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with training data and test on testing_data, and output trace to file\n",
    "\n",
    "def NaiveBayesAlgorithmTestPerformanceWithEvaluationOutputToFile(training_data, testing_data, smoothing_value, which_vocab):\n",
    "    \n",
    "    labels = ['yes', 'no']\n",
    "    \n",
    "    file_name = \"\"\n",
    "    \n",
    "    vocab = []\n",
    "    \n",
    "    if which_vocab == 'original':\n",
    "        vocab = buildOriginalVocabulary(training_data)\n",
    "        file_name = \"eval_NB-BOW-OV.txt\"\n",
    "    elif which_vocab == 'filtered':\n",
    "        vocab = buildFilteredVocabulary(training_data)\n",
    "        file_name = \"eval_NB-BOW-FV.txt\"\n",
    "    else:\n",
    "        assert(False)\n",
    "    \n",
    "    cond_probs, probs = TrainNaiveBayesAlgorithm(labels, vocab, training_data, smoothing_value)\n",
    "\n",
    "    f = open(file_name, 'w')\n",
    "          \n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "    \n",
    "    yes_true_positive_count = 0\n",
    "    yes_false_positive_count = 0\n",
    "    yes_false_negative_count = 0\n",
    "    yes_true_negative_count = 0\n",
    "    \n",
    "    no_true_positive_count = 0\n",
    "    no_false_positive_count = 0\n",
    "    no_false_negative_count = 0\n",
    "    no_true_negative_count = 0\n",
    "    \n",
    "    for data in testing_data:\n",
    "        scores = TestNaiveBayesAlgorithm(labels, probs, cond_probs, vocab, data[1])\n",
    "        \n",
    "        isYes = False\n",
    "        if scores['yes']>scores['no']:\n",
    "            yes_no_label = 'yes'\n",
    "            isYes = True\n",
    "            \n",
    "        if isYes and data[2] == 'yes':\n",
    "            correct_count = correct_count + 1\n",
    "            correct_status = 'correct'\n",
    "            yes_true_positive_count = yes_true_positive_count + 1\n",
    "            no_true_negative_count = no_true_negative_count + 1\n",
    "        elif not isYes and data[2] == 'no':\n",
    "            correct_count = correct_count + 1\n",
    "            correct_status = 'correct'\n",
    "            yes_true_negative_count = yes_true_negative_count + 1\n",
    "            no_true_positive_count = no_true_positive_count + 1\n",
    "        elif not isYes and data[2] == 'yes':\n",
    "            incorrect_count = incorrect_count + 1\n",
    "            yes_false_negative_count = yes_false_negative_count + 1\n",
    "            no_false_positive_count = no_false_positive_count + 1\n",
    "        elif isYes and data[2] == 'no':\n",
    "            incorrect_count = incorrect_count + 1\n",
    "            yes_false_positive_count = yes_false_positive_count + 1\n",
    "            no_false_negative_count = no_false_negative_count + 1\n",
    "            \n",
    "    accuracy = correct_count/len(testing_data)\n",
    "    yes_precision = yes_true_positive_count/(yes_true_positive_count+yes_false_positive_count)\n",
    "    no_precision = no_true_positive_count/(no_true_positive_count+no_false_positive_count)\n",
    "    yes_recall = yes_true_positive_count/(yes_true_positive_count+yes_false_negative_count)\n",
    "    no_recall = no_true_positive_count/(no_true_positive_count+no_false_negative_count)\n",
    "    yes_f1 = 2*yes_precision*yes_recall/(yes_precision + yes_recall)\n",
    "    no_f1 = 2*no_precision*no_recall/(no_precision + no_recall)\n",
    "            \n",
    "    f.write(str(round(accuracy,4)) + \"\\n\")\n",
    "    f.write(str(round(yes_precision, 4)) + \"  \" + str(round(no_precision,4))  + \"\\n\" )\n",
    "    f.write(str(round(yes_recall,4)) + \"  \" + str(round(no_recall,4)) + \"\\n\")\n",
    "    f.write(str(round(yes_f1,4)) + \"  \" + str(round(no_f1,4)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of filtered V 1193\n",
      "size of original V 4304\n"
     ]
    }
   ],
   "source": [
    "NaiveBayesAlgorithmTestPerformanceWithEvaluationOutputToFile(training_data, testing_data, 0.01, 'filtered')\n",
    "NaiveBayesAlgorithmTestPerformanceWithEvaluationOutputToFile(training_data, testing_data, 0.01, 'original')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
