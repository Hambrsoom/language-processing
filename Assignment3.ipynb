{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE*: Functions were built with the assumption that data files follow the tweet_id, text, q1_label, q2_label, q3_label, q4_label, q5_label, q6_label, q7_label format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare function to import data from TSV file\n",
    "\n",
    "import csv\n",
    "\n",
    "def importTSV(file_name):\n",
    "    tsv_file = open(file_name, encoding=\"utf8\")\n",
    "    read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "        \n",
    "    training_data = []\n",
    "    \n",
    "    for row in read_tsv:\n",
    "        training_data.append(row)\n",
    "        \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare function to convert data to lowercase\n",
    "\n",
    "def convertToLowerCase(training_data):\n",
    "    for row in training_data:\n",
    "        row[1] = row[1].lower()\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare function to build ORIGINAL VOCABULARY\n",
    "\n",
    "def buildOriginalVocabulary(training_data):\n",
    "    vocab = set()\n",
    "    training_data.pop(0)\n",
    "    for row in training_data:\n",
    "        for word in row[1].split():\n",
    "            vocab.add(word)\n",
    "        \n",
    "    vocab = list(vocab)\n",
    "    vocab.sort()\n",
    "        \n",
    "    return vocab\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare function to build FILTERED VOCABULARY (only words appearing twice)\n",
    "\n",
    "def buildFilteredVocabulary(training_data):\n",
    "    vocab = {}\n",
    "    training_data.pop(0)\n",
    "    for row in training_data:\n",
    "        for word in row[1].split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1\n",
    "            else:\n",
    "                vocab[word] = vocab[word] + 1\n",
    "    \n",
    "    for word in list(vocab):\n",
    "        if vocab[word] == 1:\n",
    "            vocab.pop(word, None) \n",
    "    \n",
    "    vocab = list(vocab)\n",
    "    vocab.sort()\n",
    "    \n",
    "    return vocab\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "\n",
    "training_data = importTSV(\"covid_training.tsv\")\n",
    "training_data = convertToLowerCase(training_data)\n",
    "#print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get original vocab\n",
    "\n",
    "og_vocab = buildOriginalVocabulary(training_data)\n",
    "#print(og_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab\n",
    "\n",
    "filtered_vocab = buildFilteredVocabulary(training_data)\n",
    "#print(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainNaiveBayesAlgorithm(labels, vocab, training_data, smoothing_value):\n",
    "    \n",
    "    cond_probs = {}\n",
    "    \n",
    "\n",
    "    \n",
    "    for label in labels: #class in classes, ie yes/no\n",
    "        \n",
    "        # init cond_probs\n",
    "        cond_probs[label] = {}\n",
    "        for word in vocab:\n",
    "            cond_probs[label][word] = smoothing_value\n",
    "            \n",
    "        total_word_count_for_label = 0\n",
    "        \n",
    "        # get word count for each word in this label\n",
    "        for data in training_data: # get prob of word given label\n",
    "            if data[2] == label: # check data that match label\n",
    "                total_word_count_for_label = total_word_count_for_label + 1        \n",
    "\n",
    "                for word in data[1].split(): # iterate through words of document\n",
    "                    if word in vocab: # only count words in vocabulary\n",
    "                        cond_probs[label][word] = cond_probs[label][word] + 1\n",
    "        \n",
    "        # divide specific word count by total word count for this label\n",
    "        for word in cond_probs[label]:\n",
    "            cond_probs[label][word] = cond_probs[label][word]/total_word_count_for_label\n",
    "        \n",
    "\n",
    "    probs = {}\n",
    "            \n",
    "    for label in labels: # get prob of label\n",
    "        count = 0\n",
    "        for data in training_data:\n",
    "            if data[2] == label:\n",
    "                count = count +1\n",
    "                \n",
    "        probs[label] =  count/len(training_data)\n",
    "    \n",
    "    return cond_probs, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestNaiveBayesAlgorithm(labels, probs, cond_probs, vocab, data_text):\n",
    "    scores = {}\n",
    "    for label in labels:\n",
    "        scores[label] = math.log10(probs[label])\n",
    "        for word in data_text.split():\n",
    "            if word in vocab:\n",
    "                scores[label] = scores[label] + math.log10(cond_probs[label][word])\n",
    "                \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 0.6206030150753769, 'no': 0.3793969849246231}\n"
     ]
    }
   ],
   "source": [
    "cond_probs, probs = TrainNaiveBayesAlgorithm(['yes', 'no'], og_vocab, training_data, 0.01)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = TestNaiveBayesAlgorithm(['yes', 'no'], probs, cond_probs, og_vocab, training_data[4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get NaiveBayesAlgorithm training performance ORIGINAL VOCABULARY\n",
    "\n",
    "def NaiveBayesAlgorithmTrainAndTestPerformance(training_data, smoothing_value, which_vocab):\n",
    "    \n",
    "    labels = ['yes', 'no']\n",
    "    \n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "    \n",
    "    vocab = []\n",
    "    \n",
    "    if which_vocab == 'original':\n",
    "        vocab = buildOriginalVocabulary(training_data)\n",
    "    elif which_vocab == 'filtered':\n",
    "        vocab = buildFilteredVocabulary(training_data)\n",
    "    \n",
    "    cond_probs, probs = TrainNaiveBayesAlgorithm(labels, vocab, training_data, smoothing_value)\n",
    "\n",
    "    for data in training_data:\n",
    "        scores = TestNaiveBayesAlgorithm(labels, probs, cond_probs, vocab, data[1])\n",
    "        \n",
    "        isYes = False\n",
    "        if scores['yes']>scores['no']:\n",
    "            isYes = True\n",
    "            \n",
    "        if isYes and data[2] == 'yes':\n",
    "            correct_count = correct_count + 1\n",
    "        elif not isYes and data[2] == 'no':\n",
    "            correct_count = correct_count + 1\n",
    "        else:\n",
    "            incorrect_count = incorrect_count + 1\n",
    "            \n",
    "\n",
    "    return correct_count, incorrect_count, len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training performance (original vocabulary): 0.9974811083123426\n",
      "The above may be overfitting, we will see about the generalized results when running against the testing\n",
      "\n",
      "Training performance (filtered vocabulary): 0.898989898989899\n"
     ]
    }
   ],
   "source": [
    "correct_count, incorrect_count, total_count = NaiveBayesAlgorithmTrainAndTestPerformance(training_data, 0.01, 'original')\n",
    "print(\"Training performance (original vocabulary): \" + str(correct_count/total_count)) \n",
    "print(\"The above may be overfitting, we will see about the generalized results when running against the testing\")\n",
    "\n",
    "print('')\n",
    "\n",
    "correct_count, incorrect_count, total_count = NaiveBayesAlgorithmTrainAndTestPerformance(training_data, 0.01, 'filtered')\n",
    "print(\"Training performance (filtered vocabulary): \" + str(correct_count/total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with training data and test on testing_data\n",
    "\n",
    "def NaiveBayesAlgorithmTestPerformance(training_data, testing_data, smoothing_value, which_vocab):\n",
    "    \n",
    "    labels = ['yes', 'no']\n",
    "    \n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "    \n",
    "    vocab = []\n",
    "    \n",
    "    if which_vocab == 'original':\n",
    "        vocab = buildOriginalVocabulary(training_data)\n",
    "    elif which_vocab == 'filtered':\n",
    "        vocab = buildFilteredVocabulary(training_data)\n",
    "    \n",
    "    cond_probs, probs = TrainNaiveBayesAlgorithm(labels, vocab, training_data, smoothing_value)\n",
    "\n",
    "    for data in testing_data:\n",
    "        scores = TestNaiveBayesAlgorithm(labels, probs, cond_probs, vocab, data[1])\n",
    "        \n",
    "        isYes = False\n",
    "        if scores['yes']>scores['no']:\n",
    "            isYes = True\n",
    "            \n",
    "        if isYes and data[2] == 'yes':\n",
    "            correct_count = correct_count + 1\n",
    "        elif not isYes and data[2] == 'no':\n",
    "            correct_count = correct_count + 1\n",
    "        else:\n",
    "            incorrect_count = incorrect_count + 1\n",
    "            \n",
    "\n",
    "    return correct_count, incorrect_count, len(testing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing performance (original vocabulary): 0.6181818181818182\n",
      "The above may be overfitting, we will see about the generalized results when running against the testing\n",
      "\n",
      "Testing performance (filtered vocabulary): 0.6545454545454545\n"
     ]
    }
   ],
   "source": [
    "testing_data = importTSV(\"covid_test_public.tsv\")\n",
    "testing_data = convertToLowerCase(testing_data)\n",
    "\n",
    "# Print performance of original vocabulary on testing data\n",
    "correct_count, incorrect_count, total_count = NaiveBayesAlgorithmTestPerformance(training_data, testing_data, 0.01, 'original')\n",
    "print(\"Testing performance (original vocabulary): \" + str(correct_count/total_count))\n",
    "print(\"The above may be overfitting, we will see about the generalized results when running against the testing\")\n",
    "\n",
    "print('')\n",
    "\n",
    "# Print performance of filtered vocabulary on testing data\n",
    "correct_count, incorrect_count, total_count = NaiveBayesAlgorithmTestPerformance(training_data, testing_data, 0.01, 'filtered')\n",
    "print(\"Testing performance (filtered vocabulary): \" + str(correct_count/total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the original vocabulary results in a performance of 61.82%, whereas the filtered vocabulary results in a performance of 65.45%. Therefore, we notice that the original vocabulary overfits the training data, and the filtered vocabulary is more generalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with training data and test on testing_data, and output trace to file\n",
    "\n",
    "def NaiveBayesAlgorithmTestPerformanceWithTraceOutputToFile(training_data, testing_data, smoothing_value, which_vocab):\n",
    "    \n",
    "    labels = ['yes', 'no']\n",
    "    \n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "    \n",
    "    file_name = \"\"\n",
    "    \n",
    "    vocab = []\n",
    "    \n",
    "    if which_vocab == 'original':\n",
    "        vocab = buildOriginalVocabulary(training_data)\n",
    "        file_name = \"trace_NB-BOW-OV.txt\"\n",
    "    elif which_vocab == 'filtered':\n",
    "        vocab = buildFilteredVocabulary(training_data)\n",
    "        file_name = \"trace_NB-BOW-FV.txt\"\n",
    "    else:\n",
    "        assert(False)\n",
    "    \n",
    "    cond_probs, probs = TrainNaiveBayesAlgorithm(labels, vocab, training_data, smoothing_value)\n",
    "\n",
    "    f = open(file_name, 'w')\n",
    "    \n",
    "    for data in testing_data:\n",
    "        scores = TestNaiveBayesAlgorithm(labels, probs, cond_probs, vocab, data[1])\n",
    "        \n",
    "        yes_no_label = 'no'\n",
    "        correct_status = 'wrong'\n",
    "        \n",
    "        isYes = False\n",
    "        if scores['yes']>scores['no']:\n",
    "            yes_no_label = 'yes'\n",
    "            isYes = True\n",
    "            \n",
    "        if isYes and data[2] == 'yes':\n",
    "            correct_count = correct_count + 1\n",
    "            correct_status = 'correct'\n",
    "        elif not isYes and data[2] == 'no':\n",
    "            correct_count = correct_count + 1\n",
    "            correct_status = 'correct'\n",
    "        else:\n",
    "            incorrect_count = incorrect_count + 1\n",
    "            \n",
    "        f.write(str(data[0]) + \"  \" + yes_no_label + \"  \" + str(scores[yes_no_label]) + \"  \" + data[2] + \"  \" + correct_status + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaiveBayesAlgorithmTestPerformanceWithTraceOutputToFile(training_data, testing_data, 0.01, 'filtered')\n",
    "NaiveBayesAlgorithmTestPerformanceWithTraceOutputToFile(training_data, testing_data, 0.01, 'original')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Evaluation Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with training data and test on testing_data, and output trace to file\n",
    "\n",
    "def NaiveBayesAlgorithmTestPerformanceWithEvaluationOutputToFile(training_data, testing_data, smoothing_value, which_vocab):\n",
    "    \n",
    "    labels = ['yes', 'no']\n",
    "    \n",
    "    file_name = \"\"\n",
    "    \n",
    "    vocab = []\n",
    "    \n",
    "    if which_vocab == 'original':\n",
    "        vocab = buildOriginalVocabulary(training_data)\n",
    "        file_name = \"eval_NB-BOW-OV.txt\"\n",
    "    elif which_vocab == 'filtered':\n",
    "        vocab = buildFilteredVocabulary(training_data)\n",
    "        file_name = \"eval_NB-BOW-FV.txt\"\n",
    "    else:\n",
    "        assert(False)\n",
    "    \n",
    "    cond_probs, probs = TrainNaiveBayesAlgorithm(labels, vocab, training_data, smoothing_value)\n",
    "\n",
    "    f = open(file_name, 'w')\n",
    "          \n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "    \n",
    "    yes_true_positive_count = 0\n",
    "    yes_false_positive_count = 0\n",
    "    yes_false_negative_count = 0\n",
    "    yes_true_negative_count = 0\n",
    "    \n",
    "    no_true_positive_count = 0\n",
    "    no_false_positive_count = 0\n",
    "    no_false_negative_count = 0\n",
    "    no_true_negative_count = 0\n",
    "    \n",
    "    for data in testing_data:\n",
    "        scores = TestNaiveBayesAlgorithm(labels, probs, cond_probs, vocab, data[1])\n",
    "        \n",
    "        isYes = False\n",
    "        if scores['yes']>scores['no']:\n",
    "            yes_no_label = 'yes'\n",
    "            isYes = True\n",
    "            \n",
    "        if isYes and data[2] == 'yes':\n",
    "            correct_count = correct_count + 1\n",
    "            correct_status = 'correct'\n",
    "            yes_true_positive_count = yes_true_positive_count + 1\n",
    "            no_true_negative_count = no_true_negative_count + 1\n",
    "        elif not isYes and data[2] == 'no':\n",
    "            correct_count = correct_count + 1\n",
    "            correct_status = 'correct'\n",
    "            yes_true_negative_count = yes_true_negative_count + 1\n",
    "            no_true_positive_count = no_true_positive_count + 1\n",
    "        elif not isYes and data[2] == 'yes':\n",
    "            incorrect_count = incorrect_count + 1\n",
    "            yes_false_negative_count = yes_false_negative_count + 1\n",
    "            no_false_positive_count = no_false_positive_count + 1\n",
    "        elif isYes and data[2] == 'no':\n",
    "            incorrect_count = incorrect_count + 1\n",
    "            yes_false_positive_count = yes_false_positive_count + 1\n",
    "            no_false_negative_count = no_false_negative_count + 1\n",
    "            \n",
    "    accuracy = correct_count/len(testing_data)\n",
    "    yes_precision = yes_true_positive_count/(yes_true_positive_count+yes_false_positive_count)\n",
    "    no_precision = no_true_positive_count/(no_true_positive_count+no_false_positive_count)\n",
    "    yes_recall = yes_true_positive_count/(yes_true_positive_count+yes_false_negative_count)\n",
    "    no_recall = no_true_positive_count/(no_true_positive_count+no_false_negative_count)\n",
    "    yes_f1 = 2*yes_precision*yes_recall/(yes_precision + yes_recall)\n",
    "    no_f1 = 2*no_precision*no_recall/(no_precision + no_recall)\n",
    "            \n",
    "    f.write(str(round(accuracy,4)) + \"\\n\")\n",
    "    f.write(str(round(yes_precision, 4)) + \"  \" + str(round(no_precision,4))  + \"\\n\" )\n",
    "    f.write(str(round(yes_recall,4)) + \"  \" + str(round(no_recall,4)) + \"\\n\")\n",
    "    f.write(str(round(yes_f1,4)) + \"  \" + str(round(no_f1,4)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaiveBayesAlgorithmTestPerformanceWithEvaluationOutputToFile(training_data, testing_data, 0.01, 'filtered')\n",
    "NaiveBayesAlgorithmTestPerformanceWithEvaluationOutputToFile(training_data, testing_data, 0.01, 'original')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
